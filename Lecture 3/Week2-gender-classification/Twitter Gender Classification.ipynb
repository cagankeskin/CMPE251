{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Gender Classification\n",
    "\n",
    "This example is taken from\n",
    " - https://www.udemy.com/machine-learning-ve-python-adan-zye-makine-ogrenmesi-4/\n",
    "\n",
    "Data:\n",
    " - https://www.kaggle.com/crowdflower/twitter-user-gender-classification\n",
    " \n",
    "Kernel\n",
    " -  https://www.kaggle.com/kanncaa1/applying-text-mining\n",
    " \n",
    "Look for a better accurarcy\n",
    " - https://www.kaggle.com/tranctan96/cleaning-data-applying-some-ml-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/uzaycetin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "en sik kullanilan 50 kelimeler: ['account', 'art', 'artist', 'best', 'book', 'business', 'com', 'day', 'don', 'family', 'fan', 'follow', 'free', 'friend', 'game', 'girl', 'god', 'good', 'http', 'ig', 'instagram', 'just', 'know', 'life', 'like', 'live', 'love', 'lover', 'make', 'marketing', 'medium', 'music', 'need', 'new', 'news', 'official', 'people', 'social', 'sport', 'student', 'thing', 'time', 'tweet', 'twitter', 'update', 'video', 'want', 'world', 'writer', 'year']\n",
      "accuracy:  0.492298213185459\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# %% import twitter data\n",
    "data = pd.read_csv(\"gender-classifier.csv\",encoding = \"latin1\")\n",
    "data = pd.concat([data.gender,data.description],axis=1)\n",
    "data.dropna(axis = 0,inplace = True)\n",
    "data.gender = [1 if each == \"female\" else 0 for each in data.gender]\n",
    "\n",
    "#%% clening data \n",
    "# regular expression RE mesela \"[^a-zA-Z]\"\n",
    "import re\n",
    "\n",
    "first_description = data.description[4]\n",
    "description = re.sub(\"[^a-zA-Z]\",\" \",first_description)  # a dan z ye ve A dan Z ye kadar olan harfleri bulma geri kalanları \" \" (space) ile degistir\n",
    "description = description.lower()   # buyuk harftan kucuk harfe cevirme\n",
    "\n",
    "# %% stopwords (irrelavent words) gereksiz kelimeler\n",
    "import nltk # natural language tool kit\n",
    "nltk.download(\"stopwords\")      # corpus diye bir kalsore indiriliyor\n",
    "from nltk.corpus import stopwords  # sonra ben corpus klasorunden import ediyorum\n",
    "\n",
    "# description = description.split()\n",
    "\n",
    "# split yerine tokenizer kullanabiliriz\n",
    "description = nltk.word_tokenize(description)\n",
    "\n",
    "# split kullanırsak \"shouldn't \" gibi kelimeler \"should\" ve \"not\" diye ikiye ayrılmaz ama word_tokenize() kullanirsak ayrilir\n",
    "# %%\n",
    "# greksiz kelimeleri cikar\n",
    "description = [ word for word in description if not word in set(stopwords.words(\"english\"))]\n",
    "  \n",
    "# %%             \n",
    "# lemmatazation loved => love   gitmeyecegim = > git\n",
    "\n",
    "import nltk as nlp\n",
    "\n",
    "lemma = nlp.WordNetLemmatizer()\n",
    "description = [ lemma.lemmatize(word) for word in description] \n",
    "\n",
    "description = \" \".join(description)\n",
    "\n",
    "#%% \n",
    "description_list = []\n",
    "for description in data.description:\n",
    "    description = re.sub(\"[^a-zA-Z]\",\" \",description)\n",
    "    description = description.lower()   # buyuk harftan kucuk harfe cevirme\n",
    "    description = nltk.word_tokenize(description)\n",
    "    #description = [ word for word in description if not word in set(stopwords.words(\"english\"))]\n",
    "    lemma = nlp.WordNetLemmatizer()\n",
    "    description = [ lemma.lemmatize(word) for word in description]\n",
    "    description = \" \".join(description)\n",
    "    description_list.append(description)\n",
    "\n",
    "# %% bag of words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer # bag of words yaratmak icin kullandigim metot\n",
    "max_features = 50\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\n",
    "\n",
    "sparce_matrix = count_vectorizer.fit_transform(description_list).toarray()  # x\n",
    "\n",
    "print(\"en sik kullanilan {} kelimeler: {}\".format(max_features,count_vectorizer.get_feature_names()))\n",
    "\n",
    "# %%\n",
    "y = data.iloc[:,0].values   # male or female classes\n",
    "x = sparce_matrix\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.1, random_state = 42)\n",
    "\n",
    "\n",
    "# %% naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(x_train,y_train)\n",
    "\n",
    "#%% prediction\n",
    "y_pred = nb.predict(x_test)\n",
    "\n",
    "print(\"accuracy: \",nb.score(y_pred.reshape(-1,1),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 2, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparce_matrix[6,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
