{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yapay Sinir Aglari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class yapay_sinir_agi():\n",
    "    def __init__(self, katmanlar):\n",
    "        self.katmanlar = katmanlar\n",
    "        self.b = [np.random.randn(k, 1) for k in self.katmanlar[1:]] # bias degerleri (ilk katman haric)\n",
    "        self.W = [np.random.randn(k2, k1) for k1, k2 in zip(self.katmanlar[:-1],self.katmanlar[1:])]\n",
    "\n",
    "    def ag(self):\n",
    "        return self.W, self.b\n",
    "    \n",
    "    def ileribesleme(self, a):\n",
    "        \"\"\"Katman katman yeni a degerleri hesaplaniyor\"\"\"\n",
    "        for w, b in zip(self.W, self.b):\n",
    "            z = np.dot(w, a)+b\n",
    "            a = self.sigmoid(z)\n",
    "        return a\n",
    "    \n",
    "    def geribesleme(self,X,y):\n",
    "        delta_b = [np.zeros(b.shape) for b in self.b]\n",
    "        delta_w = [np.zeros(w.shape) for w in self.W]\n",
    "        a = X; A, Z = [a], []  # A, Z degerleri\n",
    "        for w, b in zip(self.W, self.b):# z ve a degerlerini depolayalim\n",
    "            z = np.dot(w, a) + b\n",
    "            a = self.sigmoid(z)\n",
    "            Z.append(z); A.append(a)\n",
    "           \n",
    "        hata = A[-1] - y # En son katmandaki hata \n",
    "        delta = hata * self.sigmoid_turevi(Z[-1])\n",
    "        delta_b[-1] = delta # Son katmanda W, b'deki degisim  \n",
    "        delta_w[-1] = delta * A[-2].T # ERROR: np.dot(delta, A[-2].T)\n",
    "        \n",
    "        for k in range(2, len(self.katmanlar)): # Hatanin geriye yayilimi\n",
    "            delta = np.dot(self.W[-k+1].T, delta) * self.sigmoid_turevi(Z[-k])\n",
    "            delta_b[-k] = delta\n",
    "            delta_w[-k] = delta * A[-k-1].T # ERROR: np.dot(delta, A[-k-1].T)\n",
    "        return (delta_b, delta_w)  \n",
    "    \n",
    "    \n",
    "    def gradyan_inis(self, X_train, y_train, alpha, number_steps):\n",
    "        for s in range(number_steps):\n",
    "            i, m = 0,X_train.shape[1]\n",
    "            X, y = X_train[:,[i]], y_train[:,[i]]\n",
    "            tum_delta_b, tum_delta_w = self.geribesleme(X,y)\n",
    "            for i in range(1,m): # Tum X kolonlari icin\n",
    "                X, y = X_train[:,[i]], y_train[:,[i]]\n",
    "                delta_b, delta_w = self.geribesleme(X,y)\n",
    "                tum_delta_b = [tdb + db for tdb, db in zip(tum_delta_b, delta_b)]\n",
    "                tum_delta_w = [tdw + dw for tdw, dw in zip(tum_delta_w, delta_w)]\n",
    "            tum_delta_b = [alpha*tdb/m for tdb in tum_delta_b]\n",
    "            tum_delta_w = [alpha*tdw/m for tdw in tum_delta_w]\n",
    "        \n",
    "        self.W = [w - dw for w, dw in zip(self.W, tum_delta_w)]\n",
    "        self.b = [b - db for b, db in zip(self.b, tum_delta_b)]\n",
    "\n",
    "    def fit(self, X_train, y_train, alpha = 0.05, number_steps = 1000):\n",
    "        # X verileri kolon=gozlem, satir=oznitelik (alistigimizin tersi)\n",
    "        X_train = X_train.T\n",
    "        if y_train.ndim == 1: y_train = y_train.reshape(1,y_train.shape[0])\n",
    "        return self.gradyan_inis(X_train, y_train, alpha, number_steps)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        if self.katmanlar[-1] == 1 : \n",
    "            tahmin = ysa.ileribesleme(X_test.T) >= 0.5  \n",
    "            t = tahmin.astype('int')\n",
    "            return t[0]\n",
    "        return [np.argmax(self.ileribesleme(x)) for x in X_test.T] \n",
    "    \n",
    "    #### Yardimci Fonksiyonlar\n",
    "    def sigmoid(self,z):\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    def sigmoid_turevi(self,z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ysa = yapay_sinir_agi(katmanlar = [64,3,1])\n",
    "ysa.fit(X_train,y_train, alpha = 0.01, number_steps = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32  2]\n",
      " [54  2]]\n"
     ]
    }
   ],
   "source": [
    "tahmin = ysa.predict(X_test)\n",
    "print(confusion_matrix(y_test,tahmin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[65  3]\n",
      " [ 8 24]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Logistic Regression\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv(\"https://raw.githubusercontent.com/uzay00/KaVe/master/Ders3/data/Social_Network_Ads.csv\")\n",
    "X = dataset.iloc[:, [2, 3]].values\n",
    "y = dataset.iloc[:, 4].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[66  2]\n",
      " [ 9 23]]\n"
     ]
    }
   ],
   "source": [
    "ysa = yapay_sinir_agi(katmanlar = [2,1])\n",
    "ysa.fit(X_train,y_train, alpha = 0.01, number_steps = 2000)\n",
    "tahmin = ysa.predict(X_test)\n",
    "print(confusion_matrix(y_test,tahmin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1b550550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiket: 0\n",
      "ogrenme kumesinin uzunlugu:  270\n",
      "test kumesinin uzunlugu:  90\n",
      "[[45  0]\n",
      " [ 0 45]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Rakamlar veri kümesini yüklüyoruz.\n",
    "from sklearn.datasets import load_digits\n",
    "#Veri kümesini etiket değerleriyle birlikte yükleyelim.\n",
    "X,y = load_digits(return_X_y=True)\n",
    "rakam1 = X[0]\n",
    "rakam1 = np.reshape(rakam1, (8,8))\n",
    "\n",
    "plt.figure(figsize= (2,2))\n",
    "plt.imshow(rakam1, cmap=\"gray_r\")\n",
    "plt.show()\n",
    "etiket1 = y[0]\n",
    "print('Etiket: ' + str(etiket1))\n",
    "\n",
    "\n",
    "# Bu veri kumesinden sadece 0 ve 1 rakamlarini secelim\n",
    "X= X[y < 2]\n",
    "y= y[y < 2]\n",
    "\n",
    "#################################################\n",
    "# Datayi train ve test olark ayir\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print(\"ogrenme kumesinin uzunlugu: \", len(X_train))\n",
    "print(\"test kumesinin uzunlugu: \", len(X_test))\n",
    "\n",
    "#################################################\n",
    "# Datayi normalize et \n",
    "#.      Standardize features by removing the mean and scaling to unit variance\n",
    "#.      Centering and scaling happen independently on each feature\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#################################################\n",
    "# yapay ogrenme\n",
    "# Agin katmanlari 2 (girdi), 3(ara) , 1(cikti) \n",
    "    # SADECE ara katman degerlerini MLPClassifier'a veriyoruz\n",
    "    # sigmoid icin activation= 'logistic' seciyoruz\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(activation= 'logistic', hidden_layer_sizes=(3),max_iter=500)\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "#################################################\n",
    "# tahminde bulun\n",
    "predictions = mlp.predict(X_test)\n",
    "# sonuclara bak\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
